{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Scraper/HTML Parser\n",
    "\n",
    "##### Please refer to the Python Requirements and Installation Guide pdf \n",
    "\n",
    "##### Purpose: The purpose of the notebook is to scrape Kununu for companies the following companies: \n",
    "\n",
    "1. __Bechtle__ : https://www.kununu.com/de/bechtle/kommentare\n",
    "2. __SAP__: https://www.kununu.com/de/sap/kommentare\n",
    "3. __IBM__: https://www.kununu.com/de/ibm-deutschland/kommentare\n",
    "4. __T-Systems__: https://www.kununu.com/de/t-systems/kommentare \n",
    "5. __Computacenter__: https://www.kununu.com/de/computacenter/kommentare\n",
    "6. __Swisscom__: https://www.kununu.com/ch/swisscom/kommentare\n",
    "7. __Cancom__: https://www.kununu.com/de/cancom/kommentare\n",
    "8. __Adesso__: https://www.kununu.com/de/adesso/kommentare\n",
    "9. __Capegemini__: https://www.kununu.com/de/capgemini/kommentare\n",
    "10. __Fujitsu__: https://www.kununu.com/de/fujitsu-technology-solutions/kommentare\n",
    "11. __Dell__: https://www.kununu.com/de/dell5/kommentare\n",
    "\n",
    "\n",
    "#### Additional Python Libraries required: \n",
    " 1.   __Selenium__ <br> \n",
    "      Link: https://pypi.org/project/selenium/ <br>\n",
    "      pip install -U selenium <br>\n",
    " 2. __Beautiful Soup__<br>\n",
    "      Link: https://pypi.org/project/beautifulsoup4/ <br>\n",
    "      pip install beautifulsoup4 <br>\n",
    " 3. __Transformers__ <br>\n",
    "      Link: https://pypi.org/project/transformers/ <br>\n",
    "      pip install transformers\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports required within the notebook:\n",
    "\n",
    "###### Please find the first_notebook_requirements.txt file within the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages for this notebook: \n",
    "\n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('always', message='Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length.')\n",
    "warnings.filterwarnings('always', message= \"UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length)\")\n",
    "warnings.simplefilter(action='always', category = FutureWarning)\n",
    "warnings.filterwarnings('always')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping Kununu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First step: Get the url link of the Kununu page that will be scraped. Below are the companies we used for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable containing the url of each of the companies that we would like to scrape: \n",
    "url_sap = 'https://www.kununu.com/de/sap/kommentare'\n",
    "url_ibm = \"https://www.kununu.com/de/ibm-deutschland/kommentare\"\n",
    "url_bechtle = \"https://www.kununu.com/de/bechtle/kommentare\"\n",
    "url_t_systems = \"https://www.kununu.com/de/t-systems/kommentare\"\n",
    "url_computacenter = \"https://www.kununu.com/de/computacenter/kommentare\"\n",
    "url_swisscom = \"https://www.kununu.com/ch/swisscom/kommentare\"\n",
    "url_cancom = \"https://www.kununu.com/de/cancom/kommentare\"\n",
    "url_adesso = \"https://www.kununu.com/de/adesso/kommentare\"\n",
    "url_capgemini = \"https://www.kununu.com/de/capgemini/kommentare\"\n",
    "url_fujitsu = \"https://www.kununu.com/de/fujitsu-technology-solutions/kommentare\"\n",
    "url_dell = \"https://www.kununu.com/de/dell5/kommentare\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following function takes in the url and scrapes the data saving it as an html parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url, html_file_name):\n",
    "    \"\"\"\n",
    "    Purpose: The purspose of the function is to take in input as url and scrape the desired url. The function\n",
    "    then return a html file with all the scraped data. \n",
    "    Parameters:\n",
    "         url:  The desired website url to be scraped\n",
    "         html_file_name: The desired name of the html file name for the scraped url\n",
    "    Return: returns a string invoking that the scraping of the desired company is completed. \n",
    "    \"\"\"\n",
    "    \n",
    "    # chromedrivermanager is required. It is an automated browser that opens and is controlled by the code within.\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install()) \n",
    "    print(\"function started\")\n",
    "    time.sleep(3)\n",
    "    url = url\n",
    "    #driver.get(url) is utilised to load the required url into the automated web browser. \n",
    "    driver.get(url) \n",
    "# The xpath below is for accepting cookies that first appear when we enter the url. \n",
    "    driver.find_element('xpath','//*[@id=\"consent-accept-button\"]').click()  \n",
    "    time.sleep(5) # this is given to wait for 5 seconds to accept the cookies\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # index__tabCount__3oL0k class contains the number of reviews generated/written for the company\n",
    "    review_cnt = soup.find('span', class_= \"index__tabCount__3oL0k\")\n",
    "    print(review_cnt)\n",
    "    review_cnt_int = int(re.sub(r'[^\\w]', '', review_cnt.text))\n",
    "#  The clicks variable below is used to store the number of clicks that will be required to load all the reviews. \n",
    "#  We give an additional of 20 clicks which computationally would not be clicked, once the break statement is implemented.\n",
    "    clicks = round(review_cnt_int/10) + 20 \n",
    "    print(clicks)\n",
    "    print(\"starting full scraper\")\n",
    "    j = 10 # current count of clicks \n",
    "    for i in range(0, clicks):\n",
    "        try:\n",
    "            driver.find_element('xpath','//*[@id=\"reviews-read-more-cta\"]').click()\n",
    "            print(j)\n",
    "            print(f'{i}th iteration running')\n",
    "            print('***************************')\n",
    "            # loading period of 4 seconds given to load all the clicks \n",
    "            time.sleep(4)\n",
    "        except:\n",
    "            break\n",
    "        j =j+10\n",
    "# Once all the reviews have been reviewed, we parse the webpage using BeautifulSoup\n",
    "    print(\"parsing html file\")\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# Identify the class containing each review and we find all of them\n",
    "    all_reviews = soup.find_all('div', class_ = 'index__reviewBlock__27gnB')\n",
    "    print(len(all_reviews))\n",
    "    print(\"saving html file\")\n",
    "# saved html outpout into directory:\n",
    "    with open(html_file_name, \"w\", encoding = 'utf-8') as file:\n",
    "        # prettify the soup object and convert it into a string\n",
    "        file.write(str(soup.prettify()))\n",
    "\n",
    "    return(\"scraping of\", html_file_name, \"done and saved to directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step was to call the function and pass each url and save the html file: \n",
    "\n",
    "The function call has been commennted. Please uncomment in order to run the scraper and save the parsed html file\n",
    "into directory. The scraping takes approximately 15 minutes per company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE UNCOMMENT ANY OF THEM TO TEST THE SCRAPER: \n",
    "# scraper(url_computacenter, \"url_computacenter.html\")\n",
    "# scraper(url_sap ,'SAP_output (1).html')\n",
    "# scraper(url_ibm , \"IBM_output.html\")\n",
    "# scraper(url_bechtle,\"bechtle_output.html\")\n",
    "# scraper(url_t_systems ,'url_t_systems_output.html')\n",
    "# scraper(url_computacenter , \"url_computacenter.html\")\n",
    "# scraper(url_swisscom , \"url_swisscom.html\")\n",
    "# scraper(url_cancom , \"url_cancom.html\")\n",
    "# scraper(url_adesso , \"url_adesso.html\")\n",
    "# scraper(url_capgemini,\"url_capegemini.html\")\n",
    "# scraper(url_fujitsu,\"url_fujitsu.html\")\n",
    "# scraper(url_dell,\"url_dell.html\"this is a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HTML Parser \n",
    "\n",
    "#### The next step is to convert the html file into structured dataframes and save them as csv files. We extract information following information which also becomes our column names in the dataframe: \n",
    "\n",
    "1. __review_date__ <br>\n",
    "2. __review_title__ <br>\n",
    "3. __review_recommendation__ <br>\n",
    "4. __review_rating__ <br>\n",
    "5. __review_employee_info__ <br>\n",
    "6. __Arbeitsatmosphäre_star__ <br>\n",
    "7. __Arbeitsatmosphäre_plain_text__ <br>\n",
    "8. __Work-Life-Balance_star__ <br>\n",
    "9. __Work-Life-Balance_plain_text__ <br>\n",
    "10. __Kollegenzusammenhalt_star__ <br>\n",
    "11. __Kollegenzusammenhalt_plain_text__ <br>\n",
    "12. __Vorgesetztenverhalten_star__ <br> \n",
    "13. __Vorgesetztenverhalten_plain_text__ <br>\n",
    "14. __Kommunikation_star__ <br>\n",
    "15. __Kommunikation_plain_text__ <br>\n",
    "16. __Gehalt/Sozialleistungen_star__ <br> \n",
    "17. __Gehalt/Sozialleistungen_plain_text__ <br>\n",
    "18. __Gut am Arbeitgeber finde ich_st__ <br>\n",
    "19. __Gut am Arbeitgeber finde ich_plain_text__ <br>\n",
    "20. __Schlecht am Arbeitgeber finde ich_star__ <br>\n",
    "21. __Schlecht am Arbeitgeber finde ich_plain_text__ <br>\n",
    "22. __Verbesserungsvorschläge_star__ <br> \n",
    "23. __Verbesserungsvorschläge_plain_text__ <br> \n",
    "24. __Image_star__ <br> \n",
    "25. __Image_plain_text__ <br> \n",
    "26. __Karriere/Weiterbildung_star__ <br>\n",
    "27. __Karriere/Weiterbildung_plain_text__ <br>\n",
    "28. __Umwelt-/Sozialbewusstsein_plain_text__ <br> \n",
    "29. __Umgang mit älteren Kollegen_star__ <br> \n",
    "30. __Umgang mit älteren Kollegen_plain_text__ <br> \n",
    "31. __Arbeitsbedingungen_star__ <br> \n",
    "32. __Arbeitsbedingungen_plain_text__ <br> \n",
    "33. __Gleichberechtigung_star__ <br> \n",
    "34. __Gleichberechtigung_plain_text__ <br> \n",
    "35. __Interessante Aufgaben_star__ <br> \n",
    "36. __Arbeitszeiten_star__ <br> \n",
    "37. __Arbeitszeiten_plain_text__ <br> \n",
    "38. __Ausbildungsvergütung_star__ <br>\n",
    "39. __Ausbildungsvergütung_plain_text__ <br> \n",
    "40. __Die Ausbilder_star__ <br> \n",
    "41. __Die Ausbilder_plain_text__ <br> \n",
    "42. __Aufgaben/Tätigkeiten_star__ <br> \n",
    "43. __Aufgaben/Tätigkeiten_plain_text__ <br> \n",
    "44. __Variation_star__ <br> \n",
    "45. __Variation_plain_text__ <br> \n",
    "46. __Respekt_star__ <br> \n",
    "47. __Respekt_plain_text__ <br> \n",
    "48. __Karrierechancen_star__ <br> \n",
    "49. __Karrierechancen_plain_text__ <br> \n",
    "50. __Spaßfaktor_star__ <br> \n",
    "51. __Spaßfaktor_plain_text__ <br> \n",
    "52. __Wie kann dich dein Arbeitgeber im Umgang mit der Corona-Situation noch besser unterstützen?_star__ <br> \n",
    "53. __Wie kann dich dein Arbeitgeber im Umgang mit der Corona-Situation noch besser unterstützen?_plain_text__ <br> \n",
    "54. __Wofür möchtest du deinen Arbeitgeber im Umgang mit der Corona-Situation loben?_star__ <br> \n",
    "55. __Wofür möchtest du deinen Arbeitgeber im Umgang mit der Corona-Situation loben?_plain_text__ <br> \n",
    "56. __Was macht dein Arbeitgeber im Umgang mit der Corona-Situation nicht gut?_star__ <br> \n",
    "57. __Was macht dein Arbeitgeber im Umgang mit der Corona-Situation nicht gut?_plain_text__ <br> \n",
    "58. __Wo siehst du Chancen für deinen Arbeitgeber mit der Corona-Situation besser umzugehen?_star__ <br> \n",
    "59. __Wo siehst du Chancen für deinen Arbeitgeber mit der Corona-Situation besser umzugehen?_plain_text__ <br>\n",
    "\n",
    "#### Such information comprises of both the stars and the textual reviews that ex-employees have written on Kununu. \n",
    "\n",
    "#### The next step is to traverse through the html file and extract the information above. For this, we have created the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(path_to_html: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Purpose: The purpose the function is to take as input the path where the html file from the above \"scraper\" function\n",
    "    is stored and open it as a html file. \n",
    "    Paramters: \n",
    "        path_to_html: document path to where the website parsed was saved\n",
    "    \n",
    "    Return: The function returns a dataframe for each of the company html files that are passed to this function.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path_to_html, encoding=\"utf8\") as f:\n",
    "        output_soup = BeautifulSoup(f, 'html.parser')\n",
    "    return output_soup.find_all(attrs = 'div', class_ = 'index__reviewBlock__27gnB')\n",
    "\n",
    "def create_dictionaries(review_soup):\n",
    "    '''\n",
    "    Purpose: The purpose of the function is to create a dictionary for each of the companies. The key being the main\n",
    "    topic of that the former employee is talking about and the value represents the stars and textual reviews that they \n",
    "    have left for the company. \n",
    "    \n",
    "    Parameter: \n",
    "        review_soup: Contains the companies html file \n",
    "    Return: Returns a dictionary. The key representing the main topic for and the value represents the review that the \n",
    "    former employee has left behind for the company. \n",
    "    '''\n",
    "    dictionary_output = {}\n",
    "    i = 0\n",
    "    for review in review_soup:\n",
    "        dictionary_output[f'review_{i}'] = {}\n",
    "        # Review header information \n",
    "        review_header = {}\n",
    "        try:\n",
    "            review_header['review_date'] = review.find(attrs = 'time', class_ = 'p-tiny-regular text-dark-63').get('datetime')\n",
    "        except:\n",
    "            review_header['review_date'] = np.NaN\n",
    "        try:\n",
    "            review_header['review_title'] = review.find(attrs = 'h3', class_ = 'index__title__2uQec h3-semibold').text\n",
    "            review_header['review_title'] = review_header['review_title'].strip()\n",
    "        except:\n",
    "            review_header['review_title'] = np.NaN\n",
    "        try:\n",
    "            review_header['review_recommendation'] = review.find(attrs = 'span', class_ = 'p-tiny-bold').text\n",
    "            review_header['review_recommendation'] = review_header['review_recommendation'].strip()\n",
    "        except:\n",
    "            review_header['review_recommendation'] = np.NaN\n",
    "        try:\n",
    "            review_header['review_rating'] = review.find(attrs = 'span', class_ = 'h3-semibold index__score__16yy9').text\n",
    "            review_header['review_rating'] = review_header['review_rating'].strip()\n",
    "            review_header['review_rating'] = float(review_header['review_rating'].replace(',', '.'))\n",
    "        except:\n",
    "            review_header['review_rating'] = np.NaN\n",
    "        try:\n",
    "            review_header['review_employee_info'] = review.find(attrs = 'b', class_ = 'index__employmentInfoBlock__27ro4 p-tiny-regular').text\n",
    "            review_header['review_employee_info'] = review_header['review_employee_info'].replace('\\n', ' ')\n",
    "            review_header['review_employee_info'] = re.sub(\"\\s\\s+\", \" \", review_header['review_employee_info'])\n",
    "        except:\n",
    "            review_header['review_employee_info'] = np.NaN\n",
    "        dictionary_output[f'review_{i}'].update(review_header)\n",
    "#       Review body information: Need to find all tags belonging to index__factor__3Z15R p-base-regular class as\n",
    "#       few features are placed within the same class\n",
    "        review_body_list = review.find_all(attrs = 'div', class_ = 'index__factor__3Z15R p-base-regular')\n",
    "        for body_item in review_body_list:\n",
    "            body_title = body_item.find(attrs = 'h4', class_ = 'index__title__W4hOp').text\n",
    "            body_title = body_title.strip()\n",
    "            try:\n",
    "                # try to find stars that have been left in the reviews \n",
    "                body_item_star = float(body_item.find(attrs = 'span', class_ = 'index__stars__2ads4 index__medium__1wpWb index__stars__3lgvx').get('data-score'))\n",
    "            except:\n",
    "                # if the stars are not present, then place NaN as a value \n",
    "                body_item_star = np.NaN\n",
    "            try:\n",
    "#                 Since sometime the feature does not contain any text, we placed a try and except in order to \n",
    "#                 retrieve the information if there is any that is present\n",
    "                body_item_plain_text = body_item.find(attrs = 'p', class_ = 'index__plainText__lgNCM').text\n",
    "                body_item_plain_text = body_item_plain_text.replace('\\n', ' ')\n",
    "                body_item_plain_text = re.sub('\\s\\s+', ' ', body_item_plain_text)\n",
    "            except:\n",
    "                body_item_plain_text = np.NaN\n",
    "            dictionary_output[f'review_{i}'][body_title + '_star'] = body_item_star\n",
    "            dictionary_output[f'review_{i}'][body_title + '_plain_text'] = body_item_plain_text\n",
    "        i += 1\n",
    "    # creates a dictionary for with key-value pairs for each company \n",
    "    return (dictionary_output)\n",
    "\n",
    "def create_df(review_dict: dict, csv_output_name: str):\n",
    "    '''\n",
    "    Purpose: The purpose of this function is to convert the dictionary of each of the companies into a dataframe.\n",
    "    \n",
    "    Parameter: \n",
    "        review_dict: Takes input the dictionary for each of the companies. \n",
    "        csv_output_name: The desired name of the csv file containing the dataframe with all the textual information.\n",
    "    '''\n",
    "    df = pd.DataFrame.from_dict(review_dict).transpose()\n",
    "    # saves the csv in the directory \n",
    "    df.to_csv(f'{csv_output_name}.csv', sep = '\\t', encoding = 'utf-8', index = True)\n",
    "    \n",
    "    \n",
    "def send_untraslated_csv(path, output_name):\n",
    "    '''\n",
    "    Purpose: The purpose of the function is to call all the functions above in this block. \n",
    "    \n",
    "    Paramters: \n",
    "        path: The path where the html file is stored. \n",
    "        output_name: The name of the csv file that contains the dataframe. \n",
    "    '''\n",
    "\n",
    "    review_soup = create_soup(path)\n",
    "    review_soup = create_dictionaries(review_soup)\n",
    "    create_df(review_dict = review_soup, csv_output_name = output_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below code to save the csv file into your directory of choice. Pass in the path of where you want the document to be saved and its corresponding name. \n",
    "\n",
    "#### We have provided a sample of how the function needs to be called. The same function had been called 11 times to derive csv files(dataframes) for each of the companies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PLEASE PROVIDE YOUR OWN PATH AND NAME OF DESIRED CSV FILE: \n",
    "# Format of calling the function: send_untraslated_csv(path, csv_file_name)\n",
    "# Here for the path, we provide the path that contains the html file. The html output can be found in the \"HTML_scraped_output\" directory/folder. \n",
    "# For instance, here, to scrape Cancom, we provided the path to the cancom html file generated from the above code. The second \n",
    "# argument refers to the csv file which will then be saved on the working directory. \n",
    "# In our submitted codes and results, the output dataframe/csv can be found in \"output_dictionaries_and_raw_csv\" folder/directory \n",
    "send_untraslated_csv(r\"C:\\Users\\jdzuc\\OneDrive\\Frankfurt School Courses\\Third Semester\\Strategic Management\\Final Project\\Strategy Final Project\\HTML_scraped_output\\url_cancom.html\", \"cancom_german_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 3. German to English Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please be aware that to execute the below function, one should have a GPU available, to increase the computational speed of the translation. Otherwise, the CPU will be used which results in a translation time of 3 hours per company csv. The below cell, checks if a GPU is detected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To translate the dataframes, FairSeqMachineTranslation from Facebook was used, please run the below cell to import the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "mname = \"facebook/wmt19-de-en\" # transformer utilzed for tokenization and decoding\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "# we used GPU for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class translation:\n",
    "    '''\n",
    "    Purpose: The purpose of this class is to take input as the dataframe that had been created from the above functions. \n",
    "    The dataframe from above has all text in german. The aim of this class is to translate all the text from the \n",
    "    columns containing text into English.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, df): \n",
    "        \n",
    "        '''\n",
    "        df: Dataframe that needs translation\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.translated_df = pd.DataFrame()\n",
    "        self.translate_col_list = ['_title', '_recommendation', '_info', '_text']\n",
    "        self.all_cols= list(self.df.columns)\n",
    "\n",
    "    def lets_translate(self, text:str):\n",
    "        '''\n",
    "        Translation of columns\n",
    "        '''\n",
    "        input = text\n",
    "        try:\n",
    "            input_ids = tokenizer.encode(input, return_tensors=\"pt\", max_length= 200000, truncation= True)# if gpu is not available please delete \".to(device)\"\n",
    "            outputs = model.generate(input_ids) # if gpu is not available please delete \".to(device)\"\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        except:\n",
    "            return \" \"\n",
    "        return decoded\n",
    "\n",
    "    def get_list_cols(self):\n",
    "        '''\n",
    "        Gets the list of columns that need translation\n",
    "        '''\n",
    "        columns_to_translate = []\n",
    "        for item in self.all_cols:\n",
    "            if item[item.rfind('_'):] in self.translate_col_list:\n",
    "                columns_to_translate.append(item)\n",
    "\n",
    "        return columns_to_translate\n",
    "\n",
    "    def execute(self):\n",
    "        '''\n",
    "         Executes everything and returns the dataframe that has been translated.\n",
    "        '''\n",
    "        for item in tqdm(self.all_cols):\n",
    "            if item not in self.get_list_cols():\n",
    "                self.translated_df[item]= self.df[item]\n",
    "            else: \n",
    "                self.translated_df[item] = self.df[item].apply(lambda x: self.lets_translate(x))\n",
    "        return self.translated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_translated(path, file_name ):\n",
    "    '''\n",
    "    Purpose: The purpose of this function is to execute the class above, retrieve the translated dataframe and save it. \n",
    "    \n",
    "    Parameters: \n",
    "        path: The path where the german version of text is placed. \n",
    "        file_name: The name of the desired translated csv file. \n",
    "    '''\n",
    "    df = pd.read_csv(path, sep = '\\t').rename(columns= {\"Unnamed: 0\": \"review_idx\"})\n",
    "    trans_obj = translation(df)\n",
    "    df_2 = trans_obj.execute()\n",
    "    df_2.to_csv()\n",
    "    df_2.to_csv(file_name, sep = '\\t', encoding = 'utf-8', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to import the csv file, by giving in the path and the name of the file. It will then translate the csv file and send back the translated dataframe. \n",
    "\n",
    "#### The csv_translated has been called 11 time for each of the companies. A sample has been provided below for the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format of the function call: \n",
    "# csv_translated(path to to untranslated file which can be found in the directory- \"output_dictionaries_and_raw_csv\", name of the translated file)\n",
    "# The translated output/file will then be saved to your working directory and the results of the translation can be \"found translated_csvs_folder\" folder/directory\n",
    "# For instance, here, we use the untranslated sap_reviews.csv to then translate and save into working directory as sap_translated.csv\n",
    "csv_translated(r\"C:\\Users\\jdzuc\\OneDrive\\Frankfurt School Courses\\Third Semester\\Strategic Management\\Final Project\\Strategy Final Project\\output_dictionaries_and_raw_csv\\sap_reviews.csv\", f\"sap_translated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Notebook\n",
    "##### Next: The next notebook to utilise is 02_sentiment_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
